---
{"dg-publish":true,"permalink":"/02-pages/大语言模型/","tags":["personal/blog","人工智能/深度学习","人工智能/大语言模型"]}
---

大模型是一种文本预测技术。也就是说我们看到的大模型回答我的问题只是一种假象。

## 早期大语言模型的技术（2017 以前）
给出一个句子，让神经网络去预测下一个词出现的概率。例如，对于句子 "Paris is a city in ..."。这个时候对于大模型来讲会有针对各个单词的预测概率：`France: 17%, and: 15%, the: 9%...`，这个时候大模型一般会选择概率最高的单词，与句子拼接。最后得到 "Paris is a city in **France**." 本质上还是[[02-pages/深度学习\|深度学习]] 的技术。

里面的各个神经元的内容，以及神经元的链接的权重最终决定了神经网络最后一层的概率分布，改变任何一个参数都会导致最后的概率分布不同。最开始每个单词的概率分布是随机的，我们可以通过一种[[反向传播算法\|反向传播算法]]来微调所有参数，使得最后一个词比较符合我们的期望（预训练），最后通过人工的强化学习来进一步调整参数，使得结果最终离我们的期望接近。

其实这就是输入法的智能推荐的原理，也是现在的大语言模型的前身。这也是为什么大模型都是支持流式输出。

## 近期的大语言模型技术
### Transformer
不再像早期模型一样一个字一个字的输出，而是一整个句子的并行处理。将 "Paris is a city in" 中的每个单元分解为一个多维向量，如下图：
![image.png](https://yelanyanyu-img-bed.oss-cn-hangzhou.aliyuncs.com/img/blog/2025/03/20250313220758.png)
这些向量从某些角度编码了相应词的含义。

Transformer 通过注意力机制，让所有的句子中的向量可以互相“交流”，随后根据上下文并行调整这些编码的含义（改变数字）。再通过一种叫做前馈神经网络（Feedforward）来提高模型的容量，来存储更多的训练成果。

最后通过多次迭代生成对最后一个单词的预测。